{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-Words (BoW) vs. Term Frequency-Inverse Document Frequency (TF-IDF):  Comparing Text Feature Extraction Techniques\n",
        "\n",
        "In this project we explore two fundamental text feature extraction techniques to understand their impact on the performance of a text classification task. Utilizing a dataset of movie reviews, we aim to classify the sentiment of each review as either positive or negative.\n",
        "\n",
        "* **Feature Extraction Implementation**: applying BoW and TF-IDF techniques to convert textual data into numerical features suitable for ML algorithms.\n",
        "* **Model Training and Evaluation**: training a simple logistic regression classifier on the features extracted by each method and evaluate their performance in terms of accuracy, precision, recall, and F1-score.\n",
        "* **Performance Comparison**: compareing the effectiveness of BoW and TF-IDF in capturing relevant information for sentiment analysis"
      ],
      "metadata": {
        "id": "VT_px6y-e6Ki"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4jzqR4IWeyp",
        "outputId": "ddb12ae3-129f-4975-f7d0-95bebc23e476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# To access ML methods\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# To access datasets and text processing methods\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# Stopwords are common words (e.g., \"the\", \"is\", \"in\") that do not contribute much to the meaning of a document\n",
        "nltk.download('stopwords') # ensures that the stopwords dataset is available in our local NLTK data directory\n",
        "from nltk.corpus import stopwords # imports the module so we can access and use the stopwords in our code\n",
        "\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load movie_reviews dataset\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "np.random.shuffle(documents)"
      ],
      "metadata": {
        "id": "kfupXMFSYEFm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "texts = [\" \".join(doc) for doc, _ in documents]\n",
        "labels = [label for _, label in documents]"
      ],
      "metadata": {
        "id": "_P7lSdZGaYLA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.25, random_state=42)\n"
      ],
      "metadata": {
        "id": "wES3u01iaZzl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for model training and evaluation\n",
        "def train_evaluate(features_train, features_test, y_train, y_test):\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(features_train, y_train)\n",
        "    predictions = model.predict(features_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, predictions, average='binary', pos_label='pos')\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "u4NWJDXBad0H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's compare two vectorization approaches: BoW vs. TF-IDF"
      ],
      "metadata": {
        "id": "_GYtO01AbhWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag-of-Words (BoW) vectorization (CountVectorizer) represents a document\n",
        "# as a vector where each dimension corresponds to a word from the vocabulary,\n",
        "# and the value in each dimension is the count of the word's occurrence in the document.\n",
        "\n",
        "bow_vectorizer = CountVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))\n",
        "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
        "X_test_bow = bow_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"Bag-of-Words Results:\")\n",
        "train_evaluate(X_train_bow, X_test_bow, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mnOGsJjbLSa",
        "outputId": "00ed7970-4e25-4e23-f9d1-4f8ac383bf9c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words Results:\n",
            "Accuracy: 0.8640, Precision: 0.8970, Recall: 0.8261, F1-score: 0.8601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF (Term Frequency-Inverse Document Frequency) vectorization (TfidfVectorizer)\n",
        "# also represents documents as vectors, but instead of raw counts, it uses TF-IDF scores\n",
        "# that reflect how important a word is to a document in a collection of documents.\n",
        "# This helps to adjust for the fact that some words appear more frequently in general\n",
        "# and may not be as meaningful in distinguishing between documents.\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english') + list(string.punctuation))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"\\nTF-IDF Results:\")\n",
        "train_evaluate(X_train_tfidf, X_test_tfidf, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_WgZzNlbkvJ",
        "outputId": "5ade6795-2d38-4749-ac10-cb53113b6e21"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Results:\n",
            "Accuracy: 0.8400, Precision: 0.8502, Recall: 0.8300, F1-score: 0.8400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BoW model performed slightly better across all metrics in this instance. This could be due to various factors, including the nature of the dataset, the distribution of words, and how well each representation captures the relevant information for the classification task."
      ],
      "metadata": {
        "id": "7VLlJoyRe2XR"
      }
    }
  ]
}