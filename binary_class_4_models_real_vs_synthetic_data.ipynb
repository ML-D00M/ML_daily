{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Comparative Analysis of Binary Classification Models on Synthetic and Real Data\n",
        "\n",
        "In this project we explore and compare the performance of various ML models on binary classification tasks using both synthetic and real-world datasets. The primary objective is to understand how different models generalize from controlled, synthetic environments to complex, real-world scenarios.\n",
        "\n",
        "Models Explored:\n",
        "* Logistic Regression\n",
        "* K-Nearest Neighbors (KNN)\n",
        "* Neural Network (MLP)\n",
        "* Decision Tree\n",
        "\n",
        "Datasets:\n",
        "* Synthetic Dataset: Generated using scikit-learn's `make_classification` function, designed to simulate a binary classification problem with clear patterns.\n",
        "* Real Dataset: Utilizes the Pima Indians Diabetes Database, a well-known dataset in the medical domain aimed at predicting the onset of diabetes based on diagnostic measures."
      ],
      "metadata": {
        "id": "zRAOomQ60QNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gMoCYixkzkhR"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First, let's try with a synthetic dataset (for simplicity)"
      ],
      "metadata": {
        "id": "KoBRr4FW1r9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ccquM1TZ0VAX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of models to train\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Neural Network': MLPClassifier(max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier()\n",
        "}"
      ],
      "metadata": {
        "id": "Pyf364Sr0oCq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train each model, predict and print performance\n",
        "for model_name, model in models.items():\n",
        "    # Define the model\n",
        "    # Model is already defined in the dictionary, so we just fit it\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Print the performance\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"{model_name} Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgFrACIK0r5F",
        "outputId": "10c3245b-a43c-4d75-f7d8-5e12e172334f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.8550\n",
            "K-Nearest Neighbors Accuracy: 0.8100\n",
            "Neural Network Accuracy: 0.8250\n",
            "Decision Tree Accuracy: 0.8800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now let's try with a real dataset\n",
        "\n",
        "One of the classic datasets used for binary classification tasks is the Pima Indians Diabetes Database, which is available on UCI and has the objective of diagnostically predicting whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. All patients here are females at least 21 years old of Pima Indian heritage."
      ],
      "metadata": {
        "id": "phkGTWOJ1jwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import some more libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "lRETPDVb0ut4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Pima Indians Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "columns = [\"NumTimesPrg\", \"PlasmaGlucose\", \"BloodP\", \"SkinThick\", \"TwoHourSerIns\", \"BMI\", \"DiabetesPedigree\", \"Age\", \"Class\"]\n",
        "data = pd.read_csv(url, names=columns)"
      ],
      "metadata": {
        "id": "pJpygDwn2R-m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into features and target variable\n",
        "X_real = data.drop('Class', axis=1)\n",
        "y_real = data['Class']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_real_scaled = scaler.fit_transform(X_real)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(X_real_scaled, y_real, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "WWbTwtKm2dPl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of (the same) models to train\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Neural Network': MLPClassifier(max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier()\n",
        "}"
      ],
      "metadata": {
        "id": "KSAVRbLJ2w0R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train each model, predict and print performance\n",
        "for model_name, model in models.items():\n",
        "    # Define the model\n",
        "    # Model is already defined in the dictionary, so we just fit it\n",
        "    model.fit(X_real_train, y_real_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_real_test)\n",
        "\n",
        "    # Print the performance\n",
        "    accuracy = accuracy_score(y_real_test, predictions)\n",
        "    print(f\"{model_name} Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1cv5Hay26O4",
        "outputId": "6d787a92-a516-4714-bc32-20c9b4f92c4c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.7532\n",
            "K-Nearest Neighbors Accuracy: 0.6883\n",
            "Neural Network Accuracy: 0.7468\n",
            "Decision Tree Accuracy: 0.7662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some general observations reagarding the models' performance:\n",
        "\n",
        "* **General Trend**: Across all models, there is a noticeable drop in accuracy when moving from synthetic to real data. This is expected as real data tends to be noisier and contain more complex patterns than synthetic data, which is often cleaner and more controlled.\n",
        "\n",
        "* **Best Performing Model**: On synthetic data, the Decision Tree performed best, whereas on real data, its performance dropped, but it still remained one of the better-performing models. This could indicate that the Decision Tree was able to capture the underlying patterns in both datasets effectively, though less so in the real data due to its complexity.\n",
        "\n",
        "* **Worst Performing Model**: K-Nearest Neighbors had the largest drop in performance on the real dataset compared to the synthetic one. This might be due to the real data having more complex and less linearly separable features, which can adversely affect distance-based models like KNN.\n",
        "\n",
        "* **Consistency of Logistic Regression and Neural Network**: Logistic Regression and Neural Networks showed a relatively smaller decrease in performance on the real dataset. This might be because these models, especially the Neural Network, are capable of modeling complex relationships and can be more robust to noise and variability in real-world data."
      ],
      "metadata": {
        "id": "onRMLNHh38A9"
      }
    }
  ]
}